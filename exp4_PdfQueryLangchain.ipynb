{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelinflorence/largelanguagemodels/blob/main/exp4_PdfQueryLangchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fsS9n8RqdjEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Query Using Langchain"
      ],
      "metadata": {
        "id": "gBtk_tC8zmC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rdAYZepFhJM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "v8fCmC-6Q3pP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xP1-3VjZdlf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"please provide your key here\"\n",
        "os.environ[\"HF_TOKEN\"]=\"please provide your key here\""
      ],
      "metadata": {
        "id": "_aQ7ps_dRJOq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('/content/10.pdf')"
      ],
      "metadata": {
        "id": "_FA1ZERdRLAM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ],
      "metadata": {
        "id": "q9AeO9cDRqMj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yGlxUMl-Rsmy",
        "outputId": "0fef5e90-53f3-4320-a69d-2a7bd492b666"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ©2023. All\\nrights reserved. Draft of February 3, 2024.\\nCHAPTER\\n10Transformers and\\nLarge Language Models\\n“How much do we know at any time? Much more, or so I believe, than we\\nknow we know. ”\\nAgatha Christie, The Moving Finger\\nFluent speakers of a language bring an enormous amount of knowledge to bear dur-\\ning comprehension and production. This knowledge is embodied in many forms,\\nperhaps most obviously in the vocabulary, the rich representations we have of words\\nand their meanings and usage. This makes the vocabulary a useful lens to explore\\nthe acquisition of knowledge from text, by both people and machines.\\nEstimates of the size of adult vocabularies vary widely both within and across\\nlanguages. For example, estimates of the vocabulary size of young adult speakers of\\nAmerican English range from 30,000 to 100,000 depending on the resources used\\nto make the estimate and the deﬁnition of what it means to know a word. What\\nis agreed upon is that the vast majority of words that mature speakers use in their\\nday-to-day interactions are acquired early in life through spoken interactions with\\ncare givers and peers, usually well before the start of formal schooling. This active\\nvocabulary is extremely limited compared to the size of the adult vocabulary (usually\\non the order of 2000 words for young speakers) and is quite stable, with very few\\nadditional words learned via casual conversation beyond this early stage. Obviously,\\nthis leaves a very large number of words to be acquired by other means.\\nA simple consequence of these facts is that children have to learn about 7 to 10\\nwords a day, every single day , to arrive at observed vocabulary levels by the time\\nthey are 20 years of age. And indeed empirical estimates of vocabulary growth in\\nlate elementary through high school are consistent with this rate. How do children\\nachieve this rate of vocabulary growth? Most of this growth is not happening through\\ndirect vocabulary instruction in school, which is not deployed at the rate that would\\nbe required to result in sufﬁcient vocabulary growth.\\nThe most likely explanation is that the bulk of this knowledge acquisition hap-\\npens as a by-product of reading, as part of the rich processing and reasoning that we\\nperform when we read. Research into the average amount of time children spend\\nreading, and the lexical diversity of the texts they read, indicate that it is possible\\nto achieve the desired rate. But the mechanism behind this rate of learning must\\nbe remarkable indeed, since at some points during learning the rate of vocabulary\\ngrowth exceeds the rate at which new words are appearing to the learner!\\nMany of these facts have motivated approaches to word learning based on the\\ndistributional hypothesis , introduced in Chapter 6. This is the idea that something\\nabout what we’re loosely calling word meanings can be learned even without any\\ngrounding in the real world, solely based on the content of the texts we encounter\\nover our lives. This knowledge is based on the complex association of words with\\nthe words they co-occur with (and with the words that those words occur with).\\nThe crucial insight of the distributional hypothesis is that the knowledge that we\\nacquire through this process can be brought to bear long after its initial acquisition.2CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nOf course, adding grounding from vision or from real-world interaction can help\\nbuild even more powerful models, but even text alone is remarkably useful.\\nIn this chapter we formalize this idea of pretraining —learning knowledge about pretraining\\nlanguage and the world from vast amounts of text—and call the resulting pretrained\\nlanguage models large language models . Large language models exhibit remark-\\nable performance on all sorts of natural language tasks because of the knowledge\\nthey learn in pretraining, and they will play a role throughout the rest of this book.\\nThey have been especially transformative for tasks where we need to produce text,\\nlike summarization, machine translation, question answering, or chatbots.\\nThe standard architecture for building large language models is the transformer . transformer\\nWe thus begin this chapter by introducing this architecture in detail. The transformer\\nmakes use of a novel mechanism called self-attention , which developed out of the\\nidea of attention that was introduced for RNNs in Chapter 9. Self-attention can\\nbe thought of a way to build contextual representations of a word’s meaning that\\nintegrate information from surrounding words, helping the model learn how words\\nrelate to each other over large spans of text.\\nWe’ll then see how to apply the transformer to language modeling, in a setting of-\\nten called causal or autoregressive language models, in which we iteratively predict\\nwords left-to-right from earlier words. These language models, like the feedforward\\nand RNN language models we have already seen, are thus self-trained: given a large\\ncorpus of text, we iteratively teach the model to guess the next word in the text from\\nthe prior words. In addition to training, we’ll introduce algorithms for generating\\ntexts, including important methods like greedy decoding ,beam search , and sam-\\npling . And we’ll talk about the components of popular large language models like\\nthe GPT family.\\nFinally, we’ll see the great power of language models: almost any NLP task\\ncan be modeled as word prediction, if we think about it in the right way. We’ll\\nwork through an example of using large language models to solve one NLP task\\nofsummarization (generating a short text that summarizes some larger document).\\nThe use of a large language model to generate text is one of the areas in which the\\nimpact of the last decade of neural algorithms for NLP has been the largest. Indeed,\\ntext generation, along with image generation and code generation, constitute a new\\narea of AI that is often called generative AI .\\nWe’ll save three more areas of large language models for the next three chapters;\\nChapter 11 will introduce the bidirectional transformer encoder and the method of\\nmasked language modeling , used for the popular BERT family of models. Chap-\\nter 12 will introduce the most powerful way to interact with large language models:\\nprompting them to perform other NLP tasks by simply giving directions or instruc-\\ntions in natural language to a transformer that is pretrained on language modeling.\\nAnd Chapter 13 will introduce the use of the encoder-decoder architecture for trans-\\nformers in the context of machine translation.\\n10.1 The Transformer: A Self-Attention Network\\nIn this section we introduce the architecture of the transformer , the algorithm that transformer\\nunderlies most modern NLP systems. When used for causal language modeling, the\\ninput to a transformer is a sequence of words, and the output is a prediction for what\\nword comes next, as well as a sequence of contextual embedding that represents\\nthe contextual meaning of each of the input words. Like the LSTMs of Chapter 9,10.1 • T HETRANSFORMER : A S ELF-ATTENTION NETWORK 3\\ntransformers are a neural architecture that can handle distant information. But unlike\\nLSTMs, transformers are not based on recurrent connections (which can be hard to\\nparallelize), which means that transformers can be more efﬁcient to implement at\\nscale.\\nTransformers are made up of stacks of transformer blocks , each of which is a\\nmultilayer network that maps sequences of input vectors (x1;:::;xn)to sequences of\\noutput vectors (z1;:::;zn)of the same length. These blocks are made by combin-\\ning simple linear layers, feedforward networks, and self-attention layers, the key self-attention\\ninnovation of transformers. Self-attention allows a network to directly extract and\\nuse information from arbitrarily large contexts. We’ll start by describing how self-\\nattention works and then return to how it ﬁts into larger transformer blocks. Finally,\\nwe’ll describe how to use the transformer block together with some input and output\\nmechanisms as a language model, to predict upcoming words from prior words in\\nthe context.\\n10.1.1 Transformers: the intuition\\nThe intuition of a transformer is that across a series of layers, we build up richer and\\nricher contextualized representations of the meanings of input words or tokens (we\\nwill refer to the input as a sequence of words for convenience, although technically\\nthe input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather\\nthan words). At each layer of a transformer, to compute the representation of a\\nword iwe combine information from the representation of iat the previous layer\\nwith information from the representations of the neighboring words. The goal is to\\nproduce a contextualized representation for each word at each position. We can think\\nof these representations as a contextualized version of the static vectors we saw in\\nChapter 6, which each represented the meaning of a word type. By contrast, our goal\\nin transformers is to produce a contextualized version, something that represents\\nwhat this word means in the particular context in which it occurs.\\nWe thus need a mechanism that tells us how to weigh and combine the represen-\\ntations of the different words from the context at the prior level in order to compute\\nour representation at this layer. This mechanism must be able to look broadly in the\\ncontext, since words have rich linguistic relationships with words that can be many\\nsentences away. Even within the sentence, words have important linguistic relation-\\nships with contextual words. Consider these examples, each exhibiting linguistic\\nrelationships that we’ll discuss in more depth in later chapters:\\n(10.1) The keys to the cabinet areon the table.\\n(10.2) The chicken crossed the road because itwanted to get to the other side.\\n(10.3) I walked along the pond , and noticed that one of the trees along the bank\\nhad fallen into the water after the storm.\\nIn (10.1), the phrase The keys is the subject of the sentence, and in English and\\nmany languages, must agree in grammatical number with the verb are; in this case\\nboth are plural. In English we can’t use a singular verb like iswith a plural sub-\\nject like keys; we’ll discuss agreement more in Chapter 17. In (10.2), the pronoun\\nitcorefers to the chicken; it’s the chicken that wants to get to the other side. We’ll\\ndiscuss coreference more in Chapter 26. In (10.3), the way we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution is from the context, in-\\ncluding words like pond andwater . We’ll discuss word senses more in Chapter 23.\\nThese helpful contextual words can be quite far way in the sentence or paragraph,4CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nso we need a mechanism that can look broadly in the context to help compute rep-\\nresentations for words.\\nSelf-attention is just such a mechanism: it allows us to look broadly in the con-\\ntext and tells us how to integrate the representation from words in that context from\\nlayer k\\x001 to build the representation for words in layer k.\\nTheanimaldidn’tcrossthestreetbecauseitwastootiredTheanimaldidn’tcrossthestreetbecauseitwastootiredLayer 6Layer 5self-attention distribution\\nFigure 10.1 The self-attention weight distribution athat is part of the computation of the\\nrepresentation for the word itat layer 6. In computing the representation for it, we attend\\ndifferently to the various words at layer 5, with darker shades indicating higher self-attention\\nvalues. Note that the transformer is attending highly to animal , a sensible result, since in this\\nexample itcorefers with the animal, and so we’d like the representation for itto draw on the\\nrepresentation for animal . Figure simpliﬁed from (Uszkoreit, 2017).\\nFig. 10.1 shows an schematic example simpliﬁed from a real transformer (Uszko-\\nreit, 2017). Here we want to compute a contextual representation for the word it, at\\nlayer 6 of the transformer, and we’d like that representation to draw on the represen-\\ntations of all the prior words, from layer 5. The ﬁgure uses color to represent the\\nattention distribution over the contextual words: the word animal has a high atten-\\ntion weight, meaning that as we are computing the representation for it, we will draw\\nmost heavily on the representation for animal . This will be useful for the model to\\nbuild a representation that has the correct meaning for it, which indeed is corefer-\\nent here with the word animal . (We say that a pronoun like itis coreferent with a\\nnoun like animal if they both refer to the same thing; we’ll return to coreference in\\nChapter 26.)\\n10.1.2 Causal or backward-looking self-attention\\nThe concept of context can be used in two ways in self-attention. In causal, or\\nbackward looking self-attention, the context is any of the prior words. In general\\nbidirectional self-attention, the context can include future words. In this chapter\\nwe focus on causal, backward looking self-attention; we’ll introduce bidirectional\\nself-attention in Chapter 11.\\nFig. 10.2 thus illustrates the ﬂow of information in a single causal, or backward\\nlooking, self-attention layer. As with the overall transformer, a self-attention layer\\nmaps input sequences (x1;:::;xn)to output sequences of the same length (a1;:::;an).\\nWhen processing each item in the input, the model has access to all of the inputs\\nup to and including the one under consideration, but no access to information about\\ninputs beyond the current one. In addition, the computation performed for each item\\nis independent of all the other computations. The ﬁrst point ensures that we can use\\nthis approach to create language models and use them for autoregressive generation,\\nand the second point means that we can easily parallelize both forward inference\\nand training of such models.10.1 • T HETRANSFORMER : A S ELF-ATTENTION NETWORK 5\\nSelf-AttentionLayerx1a1\\nx2a2a3a4a5\\nx3x4x5\\nFigure 10.2 Information ﬂow in a causal (or masked) self-attention model. In processing\\neach element of the sequence, the model attends to all the inputs up to, and including, the\\ncurrent one. Unlike RNNs, the computations at each time step are independent of all the\\nother steps and therefore can be performed in parallel.\\n10.1.3 Self-attention more formally\\nWe’ve given the intuition of self-attention (as a way to compute representations of a\\nword at a given layer by integrating information from words at the previous layer)\\nand we’ve deﬁned context as all the prior words in the input. Let’s now introduce\\nthe self-attention computation itself.\\nThe core intuition of attention is the idea of comparing an item of interest to a\\ncollection of other items in a way that reveals their relevance in the current context.\\nIn the case of self-attention for language, the set of comparisons are to other words\\n(or tokens) within a given sequence. The result of these comparisons is then used to\\ncompute an output sequence for the current input sequence. For example, returning\\nto Fig. 10.2, the computation of a3is based on a set of comparisons between the\\ninput x3and its preceding elements x1andx2, and to x3itself.\\nHow shall we compare words to other words? Since our representations for\\nwords are vectors, we’ll make use of our old friend the dot product that we used\\nfor computing word similarity in Chapter 6, and also played a role in attention in\\nChapter 9. Let’s refer to the result of this comparison between words iand jas a\\nscore (we’ll be updating this equation to add attention to the computation of this\\nscore):\\nVerson 1: score(xi;xj) = xi\\x01xj (10.4)\\nThe result of a dot product is a scalar value ranging from \\x00¥to¥, the larger\\nthe value the more similar the vectors that are being compared. Continuing with our\\nexample, the ﬁrst step in computing y3would be to compute three scores: x3\\x01x1,\\nx3\\x01x2andx3\\x01x3. Then to make effective use of these scores, we’ll normalize them\\nwith a softmax to create a vector of weights, ai j, that indicates the proportional\\nrelevance of each input to the input element ithat is the current focus of attention.\\nai j=softmax (score(xi;xj))8j\\x14i (10.5)\\n=exp(score(xi;xj))Pi\\nk=1exp(score(xi;xk))8j\\x14i (10.6)\\nOf course, the softmax weight will likely be highest for the current focus element\\ni, since vecx iis very similar to itself, resulting in a high dot product. But other\\ncontext words may also be similar to i, and the softmax will also assign some weight\\nto those words.\\nGiven the proportional scores in a, we generate an output value aiby summing6CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nthe inputs seen so far, each weighted by its avalue.\\nai=X\\nj\\x14iai jxj (10.7)\\nThe steps embodied in Equations 10.4 through 10.7 represent the core of an\\nattention-based approach: a set of comparisons to relevant items in some context,\\na normalization of those scores to provide a probability distribution, followed by a\\nweighted sum using this distribution. The output ais the result of this straightfor-\\nward computation over the inputs.\\nThis kind of simple attention can be useful, and indeed we saw in Chapter 9\\nhow to use this simple idea of attention for LSTM-based encoder-decoder models\\nfor machine translation. But transformers allow us to create a more sophisticated\\nway of representing how words can contribute to the representation of longer inputs.\\nConsider the three different roles that each input embedding plays during the course\\nof the attention process.\\n• As the current focus of attention when being compared to all of the other\\npreceding inputs. We’ll refer to this role as a query . query\\n• In its role as a preceding input being compared to the current focus of atten-\\ntion. We’ll refer to this role as a key. key\\n• And ﬁnally, as a value used to compute the output for the current focus of value\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ,WK, and WV. These weights will be used to project each input vector xiinto\\na representation of its role as a key, query, or value.\\nqi=xiWQ;ki=xiWK;vi=xiWV(10.8)\\nThe inputs xand outputs yof transformers, as well as the intermediate vectors after\\nthe various layers like the attention output vector a, all have the same dimensionality\\n1\\x02d. We’ll have a dimension dkfor the key and query vectors, and a separate\\ndimension dvfor the value vectors. In the original transformer work (Vaswani et al.,\\n2017), dwas 512, dkanddvwere both 64. The shapes of the transform matrices are\\nthenWQ2Rd\\x02dk,WK2Rd\\x02dk, and WV2Rd\\x02dv.\\nGiven these projections, the score between a current focus of attention, xi, and\\nan element in the preceding context, xj, consists of a dot product between its query\\nvector qiand the preceding element’s key vectors kj. This dot product has the right\\nshape since both the query and the key are of dimensionality 1 \\x02dk. Let’s update\\nour previous comparison calculation to reﬂect this, replacing Eq. 10.4 with Eq. 10.9:\\nVerson 2: score(xi;xj) = qi\\x01kj (10.9)\\nThe ensuing softmax calculation resulting in ai;jremains the same, but the output\\ncalculation for aiis now based on a weighted sum over the value vectors v.\\nai=X\\nj\\x14iai jvj (10.10)\\nAgain, the softmax weight ai jwill likely be highest for the current focus element\\ni, and so the value for yiwill be most inﬂuenced by vi. But the model will also pay\\nattention to other contextual words if they are similar to i, allowing their values to10.1 • T HETRANSFORMER : A S ELF-ATTENTION NETWORK 7\\n6. Sum the weighted value vectors4. Turn into weights via softmaxa3\\n1. Generate key, query, value vectors2. Compare x3’s query withthe keys for x1, x2, and x3Output of self-attention\\nWk\\nWvWqx1kqvx3kqvx2kqv\\n××\\nWkWkWqWqWvWv5. Weigh each value vector÷dk3. Divide score by dk÷dk÷dk\\nFigure 10.3 Calculating the value of a3, the third element of a sequence using causal (left-\\nto-right) self-attention.\\nalso inﬂuence the ﬁnal value of vj. Context words that are not similar to iwill have\\ntheir values downweighted and won’t contribute to the ﬁnal value.\\nThere is one ﬁnal part of the self-attention model. The result of a dot product\\ncan be an arbitrarily large (positive or negative) value. Exponentiating large values\\ncan lead to numerical issues and to an effective loss of gradients during training. To\\navoid this, we scale down the result of the dot product, by dividing it by a factor\\nrelated to the size of the embeddings. A typical approach is to divide by the square\\nroot of the dimensionality of the query and key vectors ( dk), leading us to update\\nour scoring function one more time, replacing Eq. 10.4 and Eq. 10.9 with Eq. 10.12.\\nHere’s a ﬁnal set of equations for computing self-attention for a single self-attention\\noutput vector aifrom a single input vector xi, illustrated in Fig. 10.3 for the case of\\ncalculating the value of the third output a3in a sequence.\\nqi=xiWQ;ki=xiWK;vi=xiWV(10.11)\\nFinal verson: score(xi;xj) =qi\\x01kjpdk(10.12)\\nai j=softmax (score(xi;xj))8j\\x14i (10.13)\\nai=X\\nj\\x14iai jvj (10.14)\\n10.1.4 Parallelizing self-attention using a single matrix X\\nThis description of the self-attention process has been from the perspective of com-\\nputing a single output at a single time step i. However, since each output, yi, is\\ncomputed independently, this entire process can be parallelized, taking advantage of8CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nefﬁcient matrix multiplication routines by packing the input embeddings of the N\\ntokens of the input sequence into a single matrix X2RN\\x02d. That is, each row of X\\nis the embedding of one token of the input. Transformers for large language models\\ncan have an input length N= 1024, 2048, or 4096 tokens, so Xhas between 1K and\\n4K rows, each of the dimensionality of the embedding d.\\nWe then multiply Xby the key, query, and value matrices (all of dimensionality\\nd\\x02d) to produce matrices Q2RN\\x02d,K2RN\\x02d, and V2RN\\x02d, containing all the\\nkey, query, and value vectors:\\nQ=XWQ;K=XWK;V=XWV(10.15)\\nGiven these matrices we can compute all the requisite query-key comparisons simul-\\ntaneously by multiplying QandK|in a single matrix multiplication (the product is\\nof shape N\\x02N; Fig. 10.4 shows a visualization). Taking this one step further, we\\ncan scale these scores, take the softmax, and then multiply the result by Vresulting\\nin a matrix of shape N\\x02d: a vector embedding representation for each token in the\\ninput. We’ve reduced the entire self-attention step for an entire sequence of Ntokens\\nto the following computation:\\nA=SelfAttention (Q;K;V) = softmax\\x12QK|\\npdk\\x13\\nV (10.16)\\n10.1.5 Masking out the future\\nThe self-attention computation as we’ve described it has a problem: the calculation\\ninQK|results in a score for each query value to every key value, including those that\\nfollow the query . This is inappropriate in the setting of language modeling: guessing\\nthe next word is pretty simple if you already know it! To ﬁx this, the elements in the\\nupper-triangular portion of the matrix are zeroed out (set to \\x00¥), thus eliminating\\nany knowledge of words that follow in the sequence. Fig. 10.4 shows this masked\\nQK|matrix. (we’ll see in Chapter 11 how to make use of words in the future for\\ntasks that need it).\\nq1•k1q2•k1q2•k2q5•k1q5•k2q5•k3q5•k4q5•k5q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞−∞−∞−∞−∞\\nFigure 10.4 The N\\x02NQK|matrix showing the qi\\x01kjvalues, with the upper-triangle\\nportion of the comparisons matrix zeroed out (set to \\x00¥, which the softmax will turn to\\nzero).\\nFig. 10.4 also makes it clear that attention is quadratic in the length of the input,\\nsince at each layer we need to compute dot products between each pair of tokens in\\nthe input. This makes it expensive for the input to a transformer to consist of very\\nlong documents (like entire novels). Nonetheless modern large language models\\nmanage to use quite long contexts of up to 4096 tokens.10.2 • M ULTIHEAD ATTENTION 9\\n10.2 Multihead Attention\\nTransformers actually compute a more complex kind of attention than the single\\nself-attention calculation we’ve seen so far. This is because the different words in a\\nsentence can relate to each other in many different ways simultaneously. For exam-\\nple, distinct syntactic, semantic, and discourse relationships can hold between verbs\\nand their arguments in a sentence. It would be difﬁcult for a single self-attention\\nmodel to learn to capture all of the different kinds of parallel relations among its in-\\nputs. Transformers address this issue with multihead self-attention layers . Thesemultihead\\nself-attention\\nlayersare sets of self-attention layers, called heads, that reside in parallel layers at the same\\ndepth in a model, each with its own set of parameters. By using these distinct sets of\\nparameters, each head can learn different aspects of the relationships among inputs\\nat the same level of abstraction.\\nTo implement this notion, each head, i, in a self-attention layer is provided with\\nits own set of key, query and value matrices: WK\\ni,WQ\\niandWV\\ni. These are used\\nto project the inputs into separate key, value, and query embeddings separately for\\neach head, with the rest of the self-attention computation remaining unchanged.\\nIn multi-head attention, as with self-attention, the model dimension dis still used\\nfor the input and output, the key and query embeddings have dimensionality dk, and\\nthe value embeddings are of dimensionality dv(again, in the original transformer\\npaper dk=dv=64,h=8, and d=512). Thus for each head i, we have weight\\nlayers WQ\\ni2Rd\\x02dk,WK\\ni2Rd\\x02dk, and WV\\ni2Rd\\x02dv, and these get multiplied by\\nthe inputs packed into Xto produce Q2RN\\x02dk,K2RN\\x02dk, and V2RN\\x02dv. The\\noutput of each of the hheads is of shape N\\x02dv, and so the output of the multi-head\\nlayer with hheads consists of hmatrices of shape N\\x02dv. To make use of these\\nmatrices in further processing, they are concatenated to produce a single output with\\ndimensionality N\\x02hdv. Finally, we use yet another linear projection WO2Rhdv\\x02d,\\nthat reshape it to the original output dimension for each token. Multiplying the\\nconcatenated N\\x02hdvmatrix output by WO2Rhdv\\x02dyields the self-attention output\\nAof shape [ N\\x02d], suitable to be passed through residual connections and layer\\nnorm.\\nQ=XWQ\\ni;K=XWK\\ni;V=XWV\\ni (10.17)\\nhead i=SelfAttention (Q;K;V) (10.18)\\nA=MultiHeadAttention (X) = (head 1\\x08head 2:::\\x08head h)WO(10.19)\\nFig. 10.5 illustrates this approach with 4 self-attention heads. In general in trans-\\nformers, the multihead layer is used instead of a self-attention layer.\\n10.3 Transformer Blocks\\nThe self-attention calculation lies at the core of what’s called a transformer block,\\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\\nally called “layer norm”).\\nFig. 10.6 illustrates a standard transformer block consisting of a single attention\\nlayer followed by a position-wise feedforward layer with residual connections and\\nlayer normalizations following each.10 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nMultihead  Attention Layer with h=4 heads…x1x2x3xNConcatenateOutputs[N x hdv]WO  [hdv x d]head1 output val [N x dv]head3 output val [N x dv]head2 output val [N x dv]head4 output val [N x dv]\\nWQ1, WK1, WV1 WQ3, WK3, WV3 WQ2, WK2, WV2 WQ4, WK4, WV4 Head 1Project fromhdv to d\\nHead 2Head 3Head 4a1a2a3aN…[N x d]\\n[N x d]\\nFigure 10.5 Multihead self-attention: Each of the multihead self-attention layers is provided with its own\\nset of key, query and value weight matrices. The outputs from each of the layers are concatenated and then\\nprojected to d, thus producing an output of the same size as the input so the attention can be followed by layer\\nnorm and feedforward and layers can be stacked.\\nMultiHead Attentionz   z                                    zz   z                                    zTransformerBlock\\nx1x2x3xn…ResidualconnectionResidualconnection++h1h2h3hn……FeedforwardLayer NormalizeLayer Normalize\\nFigure 10.6 A transformer block showing all the layers.\\nFeedforward layer The feedforward layer contains Nposition-wise networks, one\\nat each position. Each is a fully-connected 2-layer network, i.e., one hidden layer,\\ntwo weight matrices, as introduced in Chapter 7. The weights are the same for each\\nposition, but the parameters are different from layer to layer. Unlike attention, the\\nfeedforward networks are independent for each position and so can be computed in\\nparallel. It is common to make the dimensionality dffof the hidden layer of the\\nfeedforward network be larger than the model dimensionality d. (For example in the\\noriginal transformer model, d=512 and dff=2048.)\\nResidual connections Residual connections are connections that pass informa-\\ntion from a lower layer to a higher layer without going through the intermediate10.3 • T RANSFORMER BLOCKS 11\\nlayer. Allowing information from the activation going forward and the gradient go-\\ning backwards to skip a layer improves learning and gives higher level layers direct\\naccess to information from lower layers (He et al., 2016). Residual connections in\\ntransformers are implemented simply by adding a layer’s input vector to its out-\\nput vector before passing it forward. In the transformer block shown in Fig. 10.6,\\nresidual connections are used with both the attention and feedforward sublayers.\\nLayer Norm These summed vectors are then normalized using layer normaliza-\\ntion (Ba et al., 2016). Layer normalization (usually called layer norm ) is one of layer norm\\nmany forms of normalization that can be used to improve training performance in\\ndeep neural networks by keeping the values of a hidden layer in a range that facil-\\nitates gradient-based training. Layer norm is a variation of the standard score, or\\nz-score, from statistics applied to a single vector in a hidden layer. The input to\\nlayer norm is a single vector, for a particular token position i, and the output is that\\nvector normalized. Thus layer norm takes as input a single vector of dimensionality\\ndand produces as output a single vector of dimensionality d. The ﬁrst step in layer\\nnormalization is to calculate the mean, m, and standard deviation, s, over the ele-\\nments of the vector to be normalized. Given a hidden layer with dimensionality dh,\\nthese values are calculated as follows.\\nm=1\\ndhdhX\\ni=1xi (10.20)\\ns=vuut1\\ndhdhX\\ni=1(xi\\x00m)2(10.21)\\nGiven these values, the vector components are normalized by subtracting the mean\\nfrom each and dividing by the standard deviation. The result of this computation is\\na new vector with zero mean and a standard deviation of one.\\n^ x=(x\\x00m)\\ns(10.22)\\nFinally, in the standard implementation of layer normalization, two learnable param-\\neters, gandb, representing gain and offset values, are introduced.\\nLayerNorm =g^ x+b (10.23)\\nPutting it all together The function computed by a transformer block can be ex-\\npressed as:\\nO=LayerNorm (X+SelfAttention (X)) (10.24)\\nH=LayerNorm (O+FFN(O)) (10.25)\\nOr we can break it down with one equation for each component computation, using\\nT(of shape [N\\x02d]) to stand for transformer and superscripts to demarcate each\\ncomputation inside the block:\\nT1=SelfAttention (X) (10.26)\\nT2=X+T1(10.27)\\nT3=LayerNorm (T2) (10.28)\\nT4=FFN(T3) (10.29)\\nT5=T4+T3(10.30)\\nH=LayerNorm (T5) (10.31)12 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nCrucially, the input and output dimensions of transformer blocks are matched so\\nthey can be stacked. Each token xiat the input to the block has dimensionality d,\\nand so the input Xand output Hare both of shape [N\\x02d].\\nTransformers for large language models stack many of these blocks, from 12\\nlayers (used for the T5 or GPT-3-small language models) to 96 layers (used for\\nGPT-3 large), to even more for more recent models. We’ll come back to this issues\\nof stacking in a bit.\\n10.4 The Residual Stream view of the Transformer Block\\nThe previous sections viewed the transformer block as applied to the entire N-token\\ninput Xof shape [N\\x02d], producing an output also of shape [N\\x02d].\\nWhile packing everything this way is a computationally efﬁcient way to imple-\\nment the transformer block, it’s not always the most perspicuous way to understand\\nwhat the transformer is doing. It’s often clearer to instead visualize what is hap-\\npening to an individual token vector xiin the input as it is processed through each\\ntransformer block. After all, most of the components of the transformer are de-\\nsigned to take a single vector of dimensionality d, corresponding to a single token,\\nand produce an output vector also of dimensionality d. For example, the feedfor-\\nward layer takes a single d-dimensional vector and produces a single d-dimensional\\nvector. Over the Ntokens in a batch, we simply use the identical feedforward layer\\nweights ( W1,W2,b1andb2) for each token i. Similarly, the layer norm function takes\\na single d-dimensional vector and produces a normalized d-dimensional version.\\nLayer Norm\\nxi+hi-1\\nLayer Norm+MultiHeadAttentionFeedforward\\nxi-1xi+1hihi+1……\\nFigure 10.7 The residual stream for token xi, showing how the input to the transformer\\nblock xiis passed up through residual connections, the output of the feedforward and multi-\\nhead attention layers are added in, and processed by layer norm, to produce the output of\\nthis block, hi, which is used as the input to the next layer transformer block. Note that of all\\nthe components, only the MultiHeadAttention component reads information from the other\\nresidual streams in the context.\\nWe can therefore talk about the processing of an individual token through all10.4 • T HERESIDUAL STREAM VIEW OF THE TRANSFORMER BLOCK 13\\nthese layers as a stream of d-dimensional representations, called the residual stream residual stream\\nand visualized in Fig. 10.7. The input at the bottom of the stream is an embedding\\nfor a token, which has dimensionality d. That initial embedding is passed up by the\\nresidual connections and the outputs of feedforward and attention layers get added\\ninto it. For each token i, at each block and layer we are passing up an embedding\\nof shape [1\\x02d]. The residual layers are constantly copying information up from\\nearlier embeddings (hence the metaphor of ‘residual stream’), so we can think of the\\nother components as adding new views of this representation back into this constant\\nstream. Feedforward networks add in a different view of the earlier embedding.\\nHere are the equations for the transformer block, now viewed from this embed-\\nding stream perspective.\\nt1\\ni=MultiHeadAttention (xi;[x1;\\x01\\x01\\x01;xN]) (10.32)\\nt2\\ni=t1\\ni+xi (10.33)\\nt3\\ni=LayerNorm (t2\\ni) (10.34)\\nt4\\ni=FFN(t3\\ni)) (10.35)\\nt5\\ni=t4\\ni+t3\\ni (10.36)\\nhi=LayerNorm (t5\\ni) (10.37)\\nNotice that the only component that takes as input information from other tokens\\n(other residual streams) is multi-head attention, which (as we see from (10.32) looks\\nat all the neighboring tokens in the context. The output from attention, however,\\nis then added into to this token’s embedding stream. In fact, Elhage et al. (2021)\\nshow that we can view attention heads as literally moving attention from the resid-\\nual stream of a neighboring token into the current stream. The high-dimensional\\nembedding space at each position thus contains information about the current to-\\nken and about neighboring tokens, albeit in different subspaces of the vector space.\\nFig. 10.8 shows a visualization of this movement.\\nToken Aresidual streamToken Bresidual stream\\nFigure 10.8 An attention head can move information from token A’s residual stream into\\ntoken B’s residual stream.\\nEquation (10.32) and following are just just the equation for a single transformer\\nblock, but the residual stream metaphor goes through all the transformer layers,\\nfrom the ﬁrst transformer blocks to the 12th, in a 12-layer transformer. At the earlier\\ntransformer blocks, the residual stream is representing the current token. At the\\nhighest transformer blocks, the residual stream is usual representing the following\\ntoken, since at the very end it’s being trained to predict the next token.\\nPre-norm vs. post-norm architecture There is an alternative form of the trans-\\nformer architecture that is commonly used because it performs better in many cases.\\nIn this prenorm transformer architecture, the layer norm happens in a slightly dif-prenorm\\ntransformer14 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nferent place: before the attention layer and before the feedforward layer, rather than\\nafterwards. Fig. 10.9 shows this architecture, with the equations below:\\nt1\\ni=LayerNorm (xi) (10.38)\\nt2\\ni=MultiHeadAttention (t1\\ni;\\x02\\nt1\\n1;\\x01\\x01\\x01;x1\\nN\\x03\\n) (10.39)\\nt3\\ni=t2\\ni+xi (10.40)\\nt4\\ni=LayerNorm (t3\\ni) (10.41)\\nt5\\ni=FFN(t4\\ni)) (10.42)\\nhi=t5\\ni+t3\\ni (10.43)\\nLayer Norm\\nxi+hi-1\\nLayer NormMultiHeadAttentionFeedforward\\nxi-1xi+1hihi+1\\n+……\\nFigure 10.9 The architecture of the prenorm transformer block. Here the nature of the\\nresidual stream, passing up information from the input, is even clearer.\\nThe prenorm transformer has one extra requirement: at the very end of the last\\n(highest) transformer block, there is a single extra layer norm that is run on the last hi\\nof each token stream (just below the language model head layer that we will deﬁne\\nbelow).\\n10.5 The input: embeddings for token and position\\nLet’s talk about where the input Xcomes from. Given a sequence of Ntokens ( Nis\\nthe context length in tokens), the matrix Xof shape [N\\x02d]has an embedding for embedding\\neach word in the context. The transformer does this by separately computing two\\nembeddings: an input token embedding, and an input positional embedding.\\nA token embedding, introduced in Chapter 7 and Chapter 9, is a vector of di-\\nmension dthat will be our initial representation for the input token. (As we pass\\nvectors up through the transformer layers in the residual stream, this embedding\\nrepresentation will change and grow, incorporating context and playing a different10.5 • T HE INPUT :EMBEDDINGS FOR TOKEN AND POSITION 15\\nrole depending on the kind of language model we are building.) The set of initial\\nembeddings are stored in the embedding matrix E, which has a row for each of the\\njVjtokens in the vocabulary. Thus each each word is a row vector of ddimensions,\\nandEhas shape [jVj\\x02d].\\nGiven an input token string like Thanks for all the we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of thanks for all the might be w=\\n[5;4000 ;10532 ;2224]. Next we use indexing to select the corresponding rows from\\nE, (row 5, row 4000, row 10532, row 2224).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent tokens as one-hot vectors of shape [1\\x02jVj], i.e., with one\\ndimension for each word in the vocabulary. Recall that in a one-hot vector all the one-hot vector\\nelements are 0 except one, the element whose dimension is the word’s index in the\\nvocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,\\nx5=1, and xi=08i6=5, as shown here:\\n[0 0 0 0 1 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplying by a one-hot vector that has only one non-zero element xi=1 simply\\nselects out the relevant row vector for word i, resulting in the embedding for word i,\\nas depicted in Fig. 10.10.\\nE|V|d1|V|d=✕550 0 0 0 1 0 0 … 0 0 0 0 1\\nFigure 10.10 Selecting the embedding vector for word V5by multiplying the embedding\\nmatrix Ewith a one-hot vector with a 1 in index 5.\\nWe can extend this idea to represent the entire token sequence as a matrix of one-\\nhot vectors, one for each of the Npositions in the transformer’s context window, as\\nshown in Fig. 10.11.\\nE|V|ddN=✕|V|N0 0 0 0 0 0 0 … 0 0 1 0 0 0 0 0 1 0 0 … 0 0 0 0 1 0 0 0 0 0 0 … 0 0 0 0 0 0 0 0 1 0 0 … 0 0 0 0 …\\nFigure 10.11 Selecting the embedding matrix for the input sequence of token ids Wby\\nmultiplying a one-hot matrix corresponding to Wby the embedding matrix E.\\nThese token embeddings are not position-dependent. To represent the position\\nof each token in the sequence, we combine these token embeddings with positional\\nembeddings speciﬁc to each position in an input sequence.positional\\nembeddings\\nWhere do we get these positional embeddings? The simplest method, called\\nabsolute position , is to start with randomly initialized embeddings correspondingabsolute\\nposition\\nto each possible input position up to some maximum length. For example, just as\\nwe have an embedding for the word ﬁsh, we’ll have an embedding for the position 3.16 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nAs with word embeddings, these positional embeddings are learned along with other\\nparameters during training. We can store them in a matrix Eposof shape [1timesN ].\\nTo produce an input embedding that captures positional information, we just add\\nthe word embedding for each input to its corresponding positional embedding. The\\nindividual token and position embeddings are both of size [1\\x02d], so their sum is also\\n[1\\x02d], This new embedding serves as the input for further processing. Fig. 10.12\\nshows the idea.\\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\\n+++++PositionEmbeddingsWordEmbeddings\\nFigure 10.12 A simple way to model position: add an embedding of the absolute position\\nto the token embedding to produce a new embedding of the same dimenionality.\\nThe ﬁnal representation of the input, the matrix X, is an [N\\x02d]matrix in which\\neach row iis the representation of the ith token in the input, computed by adding\\nE[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],\\nthe positional embedding of position i.\\nA potential problem with the simple absolute position embedding approach is\\nthat there will be plenty of training examples for the initial positions in our inputs\\nand correspondingly fewer at the outer length limits. These latter embeddings may\\nbe poorly trained and may not generalize well during testing. An alternative ap-\\nproach to absolute positional embeddings is to choose a static function that maps\\ninteger inputs to real-valued vectors in a way that captures the inherent relation-\\nships among the positions. That is, it captures the fact that position 4 in an input is\\nmore closely related to position 5 than it is to position 17. A combination of sine\\nand cosine functions with differing frequencies was used in the original transformer\\nwork. Even more complex positional embedding methods exist, such as ones that\\nrepresent relative position instead of absolute position, often implemented in the\\nattention mechanism at each layer rather than being added once at the initial input.\\n10.6 The Language Modeling Head\\nThe last component of the transformer we must introduce is the language modeling\\nhead . When we apply pretrained transformer models to various tasks, we use thelanguage\\nmodeling head\\nterm head to mean the additional neural circuitry we add on top of the basic trans- head\\nformer architecture to enable that task. The language modeling head is the circuitry\\nwe need to do language modeling.\\nRecall that language models, from the simple n-gram models of Chapter 3 through\\nthe feedforward and RNN language models of Chapter 7 and Chapter 9, are word\\npredictors. Given a context of words, they assign a probability to each possible next10.6 • T HELANGUAGE MODELING HEAD 17\\nword. For example, if the preceding context is “Thanks for all the” and we want to\\nknow how likely the next word is “ﬁsh” we would compute:\\nP(ﬁshjThanks for all the )\\nLanguage models give us the ability to assign such a conditional probability to every\\npossible next word, giving us a distribution over the entire vocabulary. The n-gram\\nlanguage models of Chapter 3 compute the probability of a word given counts of\\nits occurrence with the n\\x001 prior words. The context is thus of size n\\x001. For\\ntransformer language models, the context is the size of the transformer’s context\\nwindow, which can be quite large: up to 2048 or even 4096 tokens for large models.\\nThe job of the language modeling head is to take the the output of the ﬁnal\\ntransformer layer from the last token Nand use it to predict the upcoming word at\\nposition N+1. Fig. 10.13 shows how to accomplish this task, taking the output of\\nthe last token at the last layer (the d-dimensional output embedding of shape [1\\x02d])\\nand producing a probability distribution over words (from which we will choose one\\nto generate).\\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary V\\nFigure 10.13 The language modeling head: the circuit at the top of a transformer that maps from the output\\nembedding for token Nfrom the last transformer layer ( hL\\nN) to a probability distribution over words in the\\nvocabulary V.\\nThe ﬁrst module in Fig. 10.13 is a linear layer, whose job is to project from the\\noutput hL\\nN, which represents the output token embedding at position Nfrom the ﬁnal\\nblock L, (hence of shape [1\\x02d]) to the logit vector, or score vector, that will have a logit\\nsingle score for each of the jVjpossible words in the vocabulary V. The logit vector\\nuis thus of dimensionality 1 \\x02jVj.\\nThis linear layer can be learned, but more commonly we tie this matrix to (the\\ntranspose of) the embedding matrix E. Recall that in weight tying , we use the weight tying\\nsame weights for two different matrices in the model. Thus at the input stage of the\\ntransformer the embedding matrix (of shape [jVj\\x02d]) is used to map from a one-hot\\nvector over the vocabulary (of shape [1\\x02jVj]) to an embedding (of shape [1\\x02d]).\\nAnd then in the language model head, ET, the transpose of the embedding matrix (of\\nshape [d\\x02jVj]) is used to map back from an embedding (shape [1\\x02d]) to a vector\\nover the vocabulary (shape [1 \\x02jVj]). In the learning process, Ewill be optimized to\\nbe good at doing both of these mappings. We therefore sometimes call the transpose\\nETtheunembedding layer because it is performing this reverse mapping. unembedding18 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nA softmax layer turns the logits uinto the probabilities yover the vocabulary.\\nu=hL\\nNET(10.44)\\ny=softmax (u) (10.45)\\nWe can use these probabilities to do things like help assign a probability to a\\ngiven text. But the most important usage to generate text, which we do by sampling\\na word from these probabilities y. We might sample the highest probability word\\n(‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec-\\ntion 10.8. In either case, whatever entry ykwe choose from the probability vector y,\\nwe generate the word that has that index k.\\nX x1 x2      …      xNMultihead Self-AttentionLayer NormFeedforwardLayer NormLayer 1h1 h2      …     hNMultihead Self-AttentionLayer NormFeedforwardLayer NormLayer 2h1 h2      …     hNMultihead Self-AttentionLayer NormFeedforwardLayer NormLayer  Lh1 h2      …     hN…Sample token to generateat position N+1wN+1\\nw1 w2      …      wNInput tokensP1       P2     …    PN        Add token + position embeddings+Language ModelHeadToken probabilitiesy1y2y|V|…\\nE[w1] E[w2]  … E[wN]\\nFigure 10.14 A ﬁnal transformer decoder-only model, stacking post-norm transformer\\nblocks and mapping from a set of input tokens w1towNto a predicted next word wN+1.\\nFig. 10.14 shows the total stacked architecture. Note that the input to the ﬁrst\\ntransformer block is represented as X, which is the Nindexed word embeddings +\\nposition embeddings, E[w]+P), but the input to all the other layers is the output H\\nfrom the layer just below the current one).\\nNow that we see all these transformer layers spread out on the page, we can point\\nout another useful feature of the unembedding layer: as a tool for interpretability of10.7 • L ARGE LANGUAGE MODELS WITH TRANSFORMERS 19\\nthe internals of the transformer that we call the logit lens (Nostalgebraist, 2020). logit lens\\nWe can take a vector from any layer of the transformer and, pretending that it is\\nthe preﬁnal embedding, simply multiply it by the unembedding layer to get logits,\\nand compute a softmax to see the distribution over words that that vector might\\nbe representing. This can be a useful window into the internal representations of\\nthe model. Since the network wasn’t trained to make the internal representations\\nfunction in this way, the logit lens doesn’t always work perfectly, but this can still\\nbe a useful trick.\\nAnyhow, the Fig. 10.14 thus sketches out the entire process of taking a series of\\nwords w1:::wNand using the model to predict the next word wN+1.\\nA terminological note before we conclude: You will sometimes see a trans-\\nformer used for this kind of unidirectional causal language model called a decoder-\\nonly model . This is because this model constitutes roughly half of the encoder-decoder-only\\nmodel\\ndecoder model for transformers that we’ll see how to apply to machine translation\\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\\nencoder-decoder architecture, and it was only later that the standard paradigm for\\ncausal language model was deﬁned by using only the decoder part of this original\\narchitecture).\\nIn the next sections we’ll introduce what kind of tasks large language models can\\nbe used for, discuss various generation methods for sampling possible next words,\\nand show how to train a transformer-based large language model. In the follow-\\ning chapters we’ll expand on these ideas to introduce ﬁne-tuning, prompting, and\\nencoder-decoder architectures for transformer-based large language models.\\n10.7 Large Language Models with Transformers\\nWe’ve now seen most of the components of a transformer for language modeling\\n(what remains is sampling andtraining , which we’ll get to in the following sec-\\ntions). Before we do that, we use this section to talk about why and how we apply\\ntransformer-based large language models to NLP tasks.\\nAll of these tasks are cases of conditional generation , the task of generating text\\nconditioned on an input piece of text, a prompt. The fact that transformers have such\\nlong contexts (1024 or even 4096 tokens) makes them very powerful for conditional\\ngeneration, because they can look back so far into the prompting text.\\nConsider the simple task of text completion, illustrated in Fig. 10.15. Here a\\nlanguage model is given a text preﬁx and is asked to generate a possible completion.\\nNote that as the generation process proceeds, the model has direct access to the\\npriming context as well as to all of its own subsequently generated outputs (at least\\nas much as ﬁts in the large context window).. This ability to incorporate the entirety\\nof the earlier context and generated outputs at each time step is the key to the power\\nof large language models built from transformers.\\nSo why should we care about predicting upcoming words? The insight of large\\nlanguage modeling is that many practical NLP tasks can be cast as word predic-\\ntion, and that a powerful-enough language model can solve them with a high degree\\nof accuracy. For example, we can cast sentiment analysis as language modeling by\\ngiving a language model a context like:\\nThe sentiment of the sentence “I like Jackie Chan” is:\\nand comparing the following conditional probability of the words “positive” and the20 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nPreﬁx TextCompletion Text\\nInputEmbeddingsTransformerBlocksSample from Softmax\\nSolongall\\nandthanksforallthe\\nthe…linear layer\\nFigure 10.15 Autoregressive text completion with transformer-based large language models.\\nword “negative” to see which is higher:\\nP(positivejThe sentiment of the sentence “I like Jackie Chan” is: )\\nP(negativejThe sentiment of the sentence “I like Jackie Chan” is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the\\nsystem is given some question and must give a textual answer. We can cast the task\\nof question answering as word prediction by giving a language model a question and\\na token like A:suggesting that an answer should come next:\\nQ: Who wrote the book ``The Origin of Species\"? A:\\nIf we ask a language model to compute\\nP(wjQ: Who wrote the book “The Origin of Species”? A: )\\nand look at which words whave high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP(wjQ: Who wrote the book “The Origin of Species”? A: Charles )\\nwe might now see that Darwin is the most probable word, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a longtext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it.\\nWe can cast summarization as language modeling by giving a large language model\\na text, and follow the text by a token like tl;dr ; this token is short for something\\nlike ‘too long; don’t read’ and in recent years people often use this token, especially\\nin informal work emails, when they are going to give a short summary. We can\\nthen do conditional generation: give the language model this preﬁx, and then ask10.7 • L ARGE LANGUAGE MODELS WITH TRANSFORMERS 21\\nit to generate the following words, one by one, and take the entire response as a\\nsummary. Fig. 10.16 shows an example of a text and a human-produced summary\\nfrom a widely-used summarization corpus consisting of CNN and Daily Mirror news\\narticles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston.\\nHe joked about shipping the stuff to friends and family in warmer states, and an idea was born.\\nHis business slogan: “Our nightmare is your dream!” At ﬁrst, ShipSnowYo sold snow packed\\ninto empty 16.9-ounce water bottles for $19.99, but the snow usually melted before it reached its\\ndestination...\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.16 Examples of articles and summaries from the CNN/Daily Mail corpus (Hermann et al., 2015),\\n(Nallapati et al., 2016).\\nIf we take this full article and append the token tl;dr , we can use this as\\nthe context to prime the generation process to produce a summary as illustrated\\nin Fig. 10.17. Again, what makes transformers able to succeed at this task (as\\ncompared, say, to the primitive n-gram language model) is that the ability of self-\\nattention to incorporate information from the large context windows means that\\nthe model has access to the original article as well as to the newly generated text\\nthroughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is onegreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output ytis chosen by computing the probability for each possible\\noutputs (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆwt=argmaxw2VP(wjw<t) (10.46)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context22 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nOriginal StoryGenerated Summary\\n…reachedKyle\\nitsdestinationKyleWaring\\nWaringonlyThe…will\\nDelimiterwilltl;dr\\nFigure 10.17 Summarization with large language models using the tl;dr token and context-based autore-\\ngressive generation.\\nis identical, and the probabilistic model is the same, greedy decoding will always re-\\nsult in generating exactly the same string. We’ll see in Chapter 13 that an extension\\nto greedy decoding called beam search works well in tasks like machine translation,\\nwhich are very constrained in that we are always generating a text in one language\\nconditioned on a very speciﬁc text in another language. In most other tasks, how-\\never, people prefer text which has been generated by more sophisticated methods,\\ncalled sampling methods , that introduce a bit more diversity into the generations.\\nWe’ll see how to do that in the next few sections.\\n10.8 Large Language Models: Generation by Sampling\\nThe core of the generation process for large language models is the task of choosing\\nthe single word to generate next based on the context and based on the probabilities\\nthat the model assigns to possible words. This task of choosing a word to generate\\nbased on the model’s probabilities is called decoding . Decoding from a language decoding\\nmodel in a left-to-right manner (or right-to-left for languages like Arabic in which\\nwe read from right to left), and thus repeatedly choosing the next word conditioned\\non our previous choices is called autoregressive generation orcausal LM genera-autoregressive\\ngeneration\\ntion.1(As we’ll see, alternatives like the masked language models of Chapter 11 are\\nnon-causal because they can predict words based on both past and future words).\\nThe most common method for decoding in large language models is sampling .\\nRecall from Chapter 3 that sampling from a model’s distribution over words means sampling\\nto choose random words according to their probability assigned by the model. That\\nis, we iteratively choose a word to generate according to its probability in context\\n1Technically an autoregressive model predicts a value at time tbased on a linear function of the values\\nat times t\\x001,t\\x002, and so on. Although language models are not linear (since they have many layers of\\nnon-linearities), we loosely refer to this generation technique as autoregressive since the word generated\\nat each time step is conditioned on the word selected by the network from the previous step.10.8 • L ARGE LANGUAGE MODELS : GENERATION BY SAMPLING 23\\nas deﬁned by the model. Thus we are more likely to generate words that the model\\nthinks have a high probability in the context and less likely to generate words that\\nthe model thinks have a low probability.\\nWe saw back in Chapter 3 on page ??how to generate text from a unigram lan-\\nguage model , by repeatedly randomly sampling words according to their probability\\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\\ngenerate text from a trained transformer language model we’ll just generalize this\\nmodel a bit: at each step we’ll sample words according to their probability condi-\\ntioned on our previous choices , and we’ll use a transformer language model as the\\nprobability model that tells us this probability.\\nWe can formalize this algorithm for generating a sequence of words W=w1;w2;:::; wN\\nuntil we hit the end-of-sequence token, using x\\x18p(x)to mean ‘choose xby sam-\\npling from the distribution p(x):\\ni 1\\nwi\\x18p(w)\\nwhile wi!= EOS\\ni i + 1\\nwi\\x18p(wijw<i)\\nThe algorithm above is called random sampling , and it turns out random sam-random\\nsampling\\npling doesn’t work well enough. The problem is that even though random sampling\\nis mostly going to generate sensible, high-probable words, there are many odd, low-\\nprobability words in the tail of the distribution, and even though each one is low-\\nprobability, if you add up all the rare words, they constitute a large enough portion\\nof the distribution that they get chosen often enough to result in generating weird\\nsentences. For this reason, instead of random sampling, we usually use sampling\\nmethods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable\\ntrading off two important factors in generation: quality anddiversity . Methods\\nthat emphasize the most probable words tend to produce generations that are rated\\nby people as more accurate, more coherent, and more factual, but also more boring\\nand more repetitive. Methods that give a bit more weight to the middle-probability\\nwords tend to be more creative and more diverse, but less factual and more likely to\\nbe incoherent or otherwise low-quality.\\n10.8.1 Top- ksampling\\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosing top-k sampling\\nthe single most probable word to generate, we ﬁrst truncate the distribution to the\\ntopkmost likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these kwords according to their renormalized\\nprobabilities. More formally:\\n1. Choose in advance a number of words k\\n2. For each word in the vocabulary V, use the language model to compute the\\nlikelihood of this word given the context p(wtjw<t)\\n3. Sort the words by their likelihood, and throw away any word that is not one of\\nthe top kmost probable words.\\n4. Renormalize the scores of the kwords to be a legitimate probability distribu-\\ntion.24 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\n5. Randomly sample a word from within these remaining kmost-probable words\\naccording to its probability.\\nWhen k=1, top- ksampling is identical to greedy decoding. Setting kto a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.8.2 Nucleus or top- psampling\\nOne problem with top- ksampling is that kis ﬁxed, but the shape of the the probabil-\\nity distribution over words differs in different contexts. If we set k=10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling ornucleus sampling (Holtzman et al., top-p sampling\\n2020), is to keep not the top kwords, but the top ppercent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P(wtjw<t), the top- pvocabulary V(p)is the smallest set of\\nwords such that\\nX\\nw2V(p)P(wjw<t)\\x15p: (10.47)\\n10.8.3 Temperature sampling\\nIntemperature sampling , we don’t truncate the distribution, but instead reshapetemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\netertbefore we normalize it by passing it through the softmax. In low-temperature\\nsampling, t2(0;1]. Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from (10.45):\\ny=softmax (u) (10.48)\\nwe instead ﬁrst divide the logits by t, computing the probability vector yas\\ny=softmax (u=t) (10.49)\\nWhy does this work? When tis close to 1 the distribution doesn’t change much.\\nBut the lower tis, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t\\x141 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As tapproaches 0 the probability of the most likely word approaches 1.10.9 • L ARGE LANGUAGE MODELS : TRAINING TRANSFORMERS 25\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t>1.\\n10.9 Large Language Models: Training Transformers\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.9.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (or self-supervision\\nself-training ) algorithm we saw in Section ??: we take a corpus of text as training\\nmaterial and at each time step task the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nLCE=\\x00X\\nw2Vyt[w]logˆyt[w] (10.50)\\nIn the case of language modeling, the correct distribution ytcomes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word. So at time tthe CE loss in (10.50) can be\\nsimpliﬁed as the negative log probability the model assigns to the next word in the\\ntraining sequence.\\nLCE(ˆyt;yt) =\\x00logˆyt[wt+1] (10.51)\\nThus at each word position tof the input, the model takes as input the correct se-\\nquence of tokens w1:t, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token wt+1. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w1:t+1to estimate the probability of\\ntoken wt+2. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing . teacher forcing\\nFig. 10.18 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. As with\\nRNNs, the loss for a training sequence is the average cross-entropy loss over the\\nentire sequence. The weights in the network are adjusted to minimize the average\\nCE loss over the training sequence via gradient descent.26 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\nInputEmbeddingsTransformerBlockSoftmax overVocabulary\\nSolongandthanksforlongandthanksforNext wordall…Loss…\\n…=Linear Layer\\nFigure 10.18 Training a transformer as a language model.\\nNote the key difference between this ﬁgure and the earlier RNN-based version\\nshown in Fig. ??. There the calculation of the outputs and the losses at each step was\\ninherently serial given the recurrence in the calculation of the hidden states. With\\ntransformers, each training item can be processed in parallel since the output for\\neach element in the sequence is computed separately.\\nLarge models are generally trained by ﬁlling the full context window (for ex-\\nample 2048 or 4096 tokens for GPT3 or GPT4) with text. If documents are shorter\\nthan this, multiple documents are packed into the window with a special end-of-text\\ntoken between them. The batch size for gradient descent is usually quite large (the\\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\\n10.9.2 Training corpora for large language models\\nLarge language models are mainly trained on text scraped from the web, augmented\\nby more carefully curated data. Because these training corpora are so large, they are\\nlikely to contain many natural examples that can be helpful for NLP tasks, such as\\nquestion and answer pairs (for example from FAQ lists), translations of sentences\\nbetween various languages, documents together with their summaries, and so on.\\nWeb text is usually taken from corpora of automatically-crawled web pages like\\nthecommon crawl , a series of snapshots of the entire web produced by the non- common crawl\\nproﬁt Common Crawl ( https://commoncrawl.org/ ) that each have billions of\\nwebpages. Various cleanups of common crawl data exist, such as the Colossal Clean\\nCrawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English\\nthat is ﬁltered in various ways (deduplicated, removing non-natural language like\\ncode, sentences with offensive words from a blocklist). What is in this data? An\\nanalysis suggests that in large part it’s patent text documents, Wikipedia, and news\\nsites (Dodge et al., 2021). Wikipedia plays a role in lots of language model training,\\nas do corpora of books. The GPT3 models, for example, are trained mostly on the\\nweb (429 billion tokens), some text from books (67 billion tokens) and Wikipedia\\n(3 billion tokens).10.10 • P OTENTIAL HARMS FROM LANGUAGE MODELS 27\\n10.9.3 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of computer used for training.\\nThat is, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scales scaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. (2020) found the following three relationships for\\nlossLas a function of the number of non-embedding parameters N, the dataset size\\nD, and the compute budget C, for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL(N) =\\x12Nc\\nN\\x13aN\\n(10.52)\\nL(D) =\\x12Dc\\nD\\x13aD\\n(10.53)\\nL(C) =\\x12Cc\\nC\\x13aC\\n(10.54)\\nThe number of (non-embedding) parameters Ncan be roughly computed as fol-\\nlows (ignoring biases, and with das the input and output dimensionality of the\\nmodel, dattnas the self-attention layer size, and dffthe size of the feedforward layer):\\nN\\x192d nlayer(2dattn+dff)\\n\\x1912nlayerd2(10.55)\\n(assuming dattn=dff=4=d)\\nThus GPT-3, with n=96 layers and dimensionality d=12288, has 12\\x0296\\x02\\n122882\\x19175 billion parameters.\\nThe values of Nc,Dc,Cc,aN,aD, and aCdepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\nsmaller amounts of data, to predict what the loss would be if we were to add more\\ndata or increase model size. Other aspects of scaling laws can also tell us how much\\ndata we need to add when scaling up a model.\\n10.10 Potential Harms from Language Models\\nLarge pretrained neural language models exhibit many of the potential harms dis-\\ncussed in Chapter 4 and Chapter 6. Many of these harms become realized when\\npretrained language models are used for any downstream task, particularly those\\n2For the initial experiment in Kaplan et al. (2020) the precise values were aN= 0.076, Nc= 8.8\\x021013\\n(parameters), aD= 0.095, Dc= 5.4\\x021013(tokens), aC= 0.050, Cc= 3.1\\x02108(petaﬂop-days).28 CHAPTER 10 • T RANSFORMERS AND LARGE LANGUAGE MODELS\\ninvolving text generation, whether question answering, machine translation, or in\\nassistive technologies like writing aids or web search query completion, or predic-\\ntive typing for email (Olteanu et al., 2020).\\nFor example, language models are prone to saying things that are false, a prob-\\nlem called hallucination . Language models are trained to generate text that is pre- hallucination\\ndictable and coherent, but the training algorithms we have seen so far don’t have any\\nway to enforce that the text that is generated is correct or true. This causes enormous\\nproblems for any application where the facts matter!\\nA second source of harm is that language models can generate toxic language . toxic language\\nGehman et al. (2020) show that even completely non-toxic prompts can lead large\\nlanguage models to output hate speech and abuse their users. Language models also\\ngenerate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;\\nSheng et al., 2019) about many demographic groups.\\nOne source of biases is the training data. Gehman et al. (2020) shows that large\\nlanguage model training datasets include toxic text scraped from banned sites. There\\nare other biases than toxicity: the training data is disproportionately generated by\\nauthors from the US and from developed countries. Such biased population samples\\nlikely skew the resulting generation toward the perspectives or topics of this group\\nalone. Furthermore, language models can amplify demographic and other biases in\\ntraining data, just as we saw for embedding models in Chapter 6.\\nLanguage models can also be used by malicious actors for generating text for\\nmisinformation , phishing, or other socially harmful activities (Brown et al., 2020).\\nMcGufﬁe and Newhouse (2020) show how large language models generate text that\\nemulates online extremists, with the risk of amplifying extremist movements and\\ntheir attempt to radicalize and recruit.\\nLanguage models also present privacy issues since they can leak information\\nabout their training data. It is thus possible for an adversary to extract training-data\\ntext from a language model such as an individual person’s name, phone number,\\nand address (Henderson et al. 2017, Carlini et al. 2021). This is a problem if large\\nlanguage models are trained on private datasets such as electronic health records.\\nRelated to privacy is the issue of copyright . Large language models are trained\\non text that is copyrighted. In some countries, like the United States, the fair use\\ndoctrine allows copyrighted content to be used to build language models, but possi-\\nbly not if they are used to generate text that competes with the market for the text\\nthey are trained on.\\nFinding ways to mitigate all these harms is an important current research area in\\nNLP. At the very least, carefully analyzing the data used to pretrain large language\\nmodels is important as a way of understanding issues of toxicity, bias, privacy, and\\nfair use, making it extremely important that language models include datasheets\\n(page ??) ormodel cards (page ??) giving full replicable information on the cor-\\npora used to train them. Open-source models can specify their exact training data.\\nRequirements that models are transparent in such ways is also in the process of being\\nincorporated into the regulations of various national governments.\\n10.11 Summary\\nThis chapter has introduced the transformer, and how it can be applied to build large\\nlanguage models. Here’s a summary of the main points that we covered:BIBLIOGRAPHICAL AND HISTORICAL NOTES 29\\n• Transformers are non-recurrent networks based on self-attention . A self-\\nattention layer maps input sequences to output sequences of the same length,\\nusing attention heads that model how the surrounding words are relevant for\\nthe processing of the current word.\\n• A transformer block consists of a single attention layer followed by a feed-\\nforward layer with residual connections and layer normalizations following\\neach. Transformer blocks can be stacked to make deeper and more powerful\\nnetworks.\\n• Language models can be built out of stacks of transformer blocks, with a linear\\nand softmax max layer at the top.\\n• Transformer-based language models have a wide context window (as wide as\\n4096 tokens for current models) allowing them to draw on enormous amounts\\nof context to predict upcoming words.\\n• Many NLP tasks—such as question answering, summarization, sentiment,\\nand machine translation—can be cast as tasks of word prediction and hence\\naddressed with Large language models.\\n• The choice of which word to generate in large language models is generally\\ndone by using a sampling algorithm.\\n• Because of their ability to be used in so many ways, language models also\\nhave the potential to cause harms. Some harms include hallucinations, bias,\\nstereotypes, misinformation and propaganda, and violations of privacy and\\ncopyright.\\nBibliographical and Historical Notes\\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\\nresearch: self-attention andmemory networks . Encoder-decoder attention, the\\nidea of using a soft weighting over the encodings of input words to inform a gen-\\nerative decoder (see Chapter 13) was developed by Graves (2013) in the context of\\nhandwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended\\nto self-attention by dropping the need for separate encoding and decoding sequences\\nand instead seeing attention as a way of weighting the tokens in collecting informa-\\ntion passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016;\\nLiu et al., 2016). Other aspects of the transformer, including the terminology of key,\\nquery, and value, came from memory networks , a mechanism for adding an ex-\\nternal read-write memory to networks, by using an embedding of a query to match\\nkeys representing content in an associative memory (Sukhbaatar et al., 2015; Weston\\net al., 2015; Graves et al., 2014).\\nMORE HISTORY TBD IN NEXT DRAFT.30 Chapter 10 • Transformers and Large Language Models\\nBa, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normal-\\nization. NeurIPS workshop .\\nBahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\\nchine translation by jointly learning to align and translate.\\nICLR 2015 .\\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\nA. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger,\\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\\nA. Radford, I. Sutskever, and D. Amodei. 2020. Language\\nmodels are few-shot learners. NeurIPS , volume 33.\\nCarlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-\\nV oss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er-\\nlingsson, et al. 2021. Extracting training data from large\\nlanguage models. 30th USENIX Security Symposium\\n(USENIX Security 21) .\\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\\nmemory-networks for machine reading. EMNLP .\\nCheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-\\nsonas: Using natural language prompts to measure stereo-\\ntypes in language models. ACL 2023 .\\nDodge, J., M. Sap, A. Marasovi ´c, W. Agnew, G. Ilharco,\\nD. Groeneveld, M. Mitchell, and M. Gardner. 2021. Doc-\\numenting large webtext corpora: A case study on the\\ncolossal clean crawled corpus. EMNLP .\\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\\nB. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-\\nSarma, D. Drain, D. Ganguli, Z. Hatﬁeld-Dodds, D. Her-\\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\\ndlish, and C. Olah. 2021. A mathematical framework for\\ntransformer circuits. White paper.\\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\\nral toxic degeneration in language models. Findings of\\nEMNLP .\\nGraves, A. 2013. Generating sequences with recurrent neural\\nnetworks. ArXiv.\\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur-\\ning machines. ArXiv.\\nHe, K., X. Zhang, S. Ren, and J. Sun. 2016. Deep residual\\nlearning for image recognition. CVPR .\\nHenderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke,\\nG. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal-\\nlenges in data-driven dialogue systems. AAAI/ACM AI\\nEthics and Society Conference .\\nHermann, K. M., T. Ko ˇcisk´y, E. Grefenstette, L. Espeholt,\\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teach-\\ning machines to read and comprehend. Proceedings of\\nthe 28th International Conference on Neural Information\\nProcessing Systems - Volume 1 . MIT Press.\\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y . Choi. 2020.\\nThe curious case of neural text degeneration. ICLR .\\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown,\\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\\nD. Amodei. 2020. Scaling laws for neural language mod-\\nels. ArXiv preprint.Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\\nS. Amir, L. Marujo, and T. Lu ´ıs. 2015. Finding function\\nin form: Compositional character models for open vocab-\\nulary word representation. EMNLP .\\nLiu, Y ., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\\nlanguage inference using bidirectional LSTM model and\\ninner-attention. ArXiv.\\nMcGufﬁe, K. and A. Newhouse. 2020. The radicalization\\nrisks of GPT-3 and advanced neural language models.\\nArXiv preprint arXiv:2009.06807.\\nNallapati, R., B. Zhou, C. dos Santos, C ¸ . Gulc ¸ehre, and\\nB. Xiang. 2016. Abstractive text summarization using\\nsequence-to-sequence RNNs and beyond. CoNLL .\\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\\npaper.\\nOlteanu, A., F. Diaz, and G. Kazai. 2020. When are search\\ncompletion suggestions problematic? CSCW .\\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer. JMLR , 21(140):1–67.\\nSheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.\\nThe woman worked as a babysitter: On biases in language\\ngeneration. EMNLP .\\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\\nEnd-to-end memory networks. NeurIPS .\\nUszkoreit, J. 2017. Transformer: A novel neural network ar-\\nchitecture for language understanding. Google Research\\nblog post, Thursday August 31, 2017.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten-\\ntion is all you need. NeurIPS .\\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory net-\\nworks. ICLR 2015 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VP6ap_PSRt7s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9GLXwH1SVOe",
        "outputId": "ba060b27-7398-4775-ea1e-65ccd2742c08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-huggingface"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2bSGBEgC-d25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "model_kwargs = {'device': 'cuda'}\n",
        "# Initialize HuggingFaceEmbeddings without the 'model' parameter\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                                   model_kwargs = model_kwargs,\n",
        "                                   encode_kwargs = encode_kwargs)\n"
      ],
      "metadata": {
        "id": "WKAeo0apDMxA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "3igYiWjISjvS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bJpE1qlSlNb",
        "outputId": "6999f97e-983a-47db-dfd4-6feb82096eb9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x78c6c2316c50>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "Po-ip1fPSonv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "iYl2PzKSSqg0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"context meaning\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "GQafhpOz4IsV",
        "outputId": "b16c2706-d0de-4829-ad45-97498555c036"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In causal, or backward looking self-attention, the context refers to any of the prior words in the sequence. This type of self-attention uses the context as a way to build a contextualized representation of the meaning of a word at a specific position in the sequence. By combining information from the representation of the word at the previous layer with information from the representations of neighboring words, a more accurate and meaningful representation of the word can be produced. Essentially, the context helps to provide a richer understanding of the word's meaning by taking into account the words that come before and after it in the sequence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how to calculate query and key\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "7sjc1Xh2SsTs",
        "outputId": "ec9836fe-1669-4aad-a13a-e95ee2b87fad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' According to the text, the query and key vectors are calculated by projecting each input vector xi into its role as a key or query using weight matrices WQ and WK, respectively. This is done using the following equations:\\nqi = xiWQ, ki = xiWK.\\nThese weight matrices are introduced by transformers to capture the three different roles that an input can have: query, key, and value. The resulting matrices Q, K, and V are then used to compute the attention output vector a using the self-attention formula:\\na = Softmax(QK^T pdk) pdk(V), where pdk is a scaling factor.\\nThis computation reduces the entire self-attention step for an entire sequence of N tokens to a single matrix multiplication.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Fo7RaEjjt9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}